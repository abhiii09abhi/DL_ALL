{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab Lab Assignment -NLP\n",
        "\n",
        "**Course Name:** [Enter Course Name]\n",
        "\n",
        "**Lab Title:** NLP Techniques for Text Classification\n",
        "\n",
        "**Student Name:**[Enter Your Name]\n",
        "\n",
        "**Student ID:**[Enter Your ID]\n",
        "\n",
        "**Date of Submission:** [Enter Date]\n",
        "\n",
        "**Group Members**: [Enter Names]\n",
        "\n",
        "\n",
        "**Objective**\n",
        "The objective of this assignment is to implement NLP preprocessing techniques and build a text classification model using machine learning techniques.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qfwaeuSgcupl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Outcomes:**\n",
        "\n",
        "1. Understand and apply NLP preprocessing techniques such as tokenization, stopword removal, stemming, and lemmatization.\n",
        "\n",
        "2. Implement text vectorization techniques such as TF-IDF and CountVectorizer.\n",
        "\n",
        "3. Develop a text classification model using a machine learning algorithm.\n",
        "\n",
        "4. Evaluate the performance of the model using suitable metrics."
      ],
      "metadata": {
        "id": "nN1DLv3exqNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Instructions:**"
      ],
      "metadata": {
        "id": "jvYJRtGTI32Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: NLP Preprocessing**\n",
        "\n",
        "**Dataset Selection:**\n",
        "\n",
        "Choose any text dataset from **Best Datasets for Text** https://en.innovatiana.com/post/best-datasets-for-text-classification Classification, such as SMS Spam Collection, IMDb Reviews, or any other relevant dataset.\n",
        "\n",
        "Download the dataset and upload it to Google Colab.\n",
        "\n",
        "Load the dataset into a Pandas DataFrame and explore its structure (e.g., check missing values, data types, and label distribution).\n",
        "\n",
        "Text Preprocessing:\n",
        "\n",
        "Convert text to lowercase.\n",
        "\n",
        "Perform tokenization using NLTK or spaCy.\n",
        "\n",
        "Remove stopwords using NLTK or spaCy.\n",
        "\n",
        "Apply stemming using PorterStemmer or SnowballStemmer.\n",
        "\n",
        "Apply lemmatization using WordNetLemmatizer.\n",
        "\n",
        "Vectorization Techniques:\n",
        "\n",
        "Convert text data into numerical format using TF-IDF and CountVectorizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "hPA-RFjFc3hF"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the delimiter and error handling strategy\n",
        "df = pd.read_csv(\"/content/Sentiment Analysis Dataset.csv\", encoding=\"ISO-8859-1\", delimiter=',', on_bad_lines='warn')  # or 'skip'\n",
        "\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "print(df[\"Sentiment\"].value_counts())\n",
        "\n",
        "df[\"SentimentText\"] = df[\"SentimentText\"].str.lower()\n",
        "print(df[[\"SentimentText\"]].head())\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(df.dtypes)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ymiYOMhdZm",
        "outputId": "a545ef2b-e3b2-4b13-b349-f6797ce42287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-3cb5af87df2a>:4: ParserWarning: Skipping line 8836: expected 4 fields, saw 5\n",
            "\n",
            "  df = pd.read_csv(\"/content/Sentiment Analysis Dataset.csv\", encoding=\"ISO-8859-1\", delimiter=',', on_bad_lines='warn')  # or 'skip'\n",
            "<ipython-input-2-3cb5af87df2a>:4: ParserWarning: Skipping line 535882: expected 4 fields, saw 7\n",
            "\n",
            "  df = pd.read_csv(\"/content/Sentiment Analysis Dataset.csv\", encoding=\"ISO-8859-1\", delimiter=',', on_bad_lines='warn')  # or 'skip'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ï»¿ItemID  Sentiment SentimentSource  \\\n",
            "0          1          0    Sentiment140   \n",
            "1          2          0    Sentiment140   \n",
            "2          3          1    Sentiment140   \n",
            "3          4          0    Sentiment140   \n",
            "4          5          0    Sentiment140   \n",
            "\n",
            "                                       SentimentText  \n",
            "0                       is so sad for my APL frie...  \n",
            "1                     I missed the New Moon trail...  \n",
            "2                            omg its already 7:30 :O  \n",
            "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
            "4           i think mi bf is cheating on me!!!   ...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1578612 entries, 0 to 1578611\n",
            "Data columns (total 4 columns):\n",
            " #   Column           Non-Null Count    Dtype \n",
            "---  ------           --------------    ----- \n",
            " 0   ï»¿ItemID        1578612 non-null  int64 \n",
            " 1   Sentiment        1578612 non-null  int64 \n",
            " 2   SentimentSource  1578612 non-null  object\n",
            " 3   SentimentText    1578612 non-null  object\n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 48.2+ MB\n",
            "None\n",
            "ï»¿ItemID          0\n",
            "Sentiment          0\n",
            "SentimentSource    0\n",
            "SentimentText      0\n",
            "dtype: int64\n",
            "Sentiment\n",
            "1    790177\n",
            "0    788435\n",
            "Name: count, dtype: int64\n",
            "                                       SentimentText\n",
            "0                       is so sad for my apl frie...\n",
            "1                     i missed the new moon trail...\n",
            "2                            omg its already 7:30 :o\n",
            "3            .. omgaga. im sooo  im gunna cry. i'...\n",
            "4           i think mi bf is cheating on me!!!   ...\n",
            "ï»¿ItemID          0\n",
            "Sentiment          0\n",
            "SentimentSource    0\n",
            "SentimentText      0\n",
            "dtype: int64\n",
            "ï»¿ItemID           int64\n",
            "Sentiment           int64\n",
            "SentimentSource    object\n",
            "SentimentText      object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhiwsJjN0z3Y",
        "outputId": "aec9c2e8-aecf-4455-89f5-8f0ed8bb7ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "NMnx-K8R04Kq",
        "outputId": "db22acc8-d894-42d9-bd4f-1d71c074bbcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenization\n",
        "    tokens = [t for t in tokens if t.isalpha()]  # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
        "    stemmed = [stemmer.stem(t) for t in tokens]  # Stemming\n",
        "    lemmatized = [lemmatizer.lemmatize(t) for t in stemmed]  # Lemmatization\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['clean_text'] = df['text_column_name'].apply(preprocess)  # Replace 'text_column_name'\n",
        "df[['text_column_name', 'clean_text']].head()\n",
        "\n"
      ],
      "metadata": {
        "id": "3Ihqe0XEiMSj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "e13e0ca6-dccd-4597-a827-33f5609c5649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text_column_name'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_column_name'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7405a671b836>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Apply preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_column_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'text_column_name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_column_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text_column_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Data:**\n",
        "\n",
        "Divide the dataset into training and testing sets (e.g., 80% training, 20% testing).\n",
        "\n",
        "**Building the Classification Model:**\n",
        "\n",
        "Train a text classification model using Logistic Regression, Naïve Bayes, or any other suitable algorithm.\n",
        "\n",
        "Implement the model using scikit-learn.\n",
        "\n",
        "**Model Evaluation:**\n",
        "\n",
        "Evaluate the model using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Use a confusion matrix to visualize the results."
      ],
      "metadata": {
        "id": "CuEXacYmc8lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for Part 2"
      ],
      "metadata": {
        "id": "-b2Z6SaydAfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Guidelines:**\n",
        "\n",
        "**Google Colab Notebook Submission:**\n",
        "\n",
        "Save your notebook as NLP_Text_Classification_YourName.ipynb.\n",
        "\n",
        "Ensure all code cells are executed, and the output is visible.\n",
        "\n",
        "Include proper documentation and comments explaining each step.\n",
        "\n",
        "**Report Submission (Optional):**\n",
        "\n",
        "Prepare a short report (2-3 pages) summarizing your approach, findings, and model performance.\n",
        "\n",
        "Upload the report along with the Colab Notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "TDlDzi_jJvYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grading Criteria:**\n",
        "\n",
        "Correct implementation of NLP preprocessing (30%)\n",
        "\n",
        "Effective use of vectorization techniques (20%)\n",
        "\n",
        "Model accuracy and performance evaluation (30%)\n",
        "\n",
        "Code clarity, documentation, and presentation (20%)"
      ],
      "metadata": {
        "id": "4xnsvv56Nsth"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wgh54EcmJmIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declaration**\n",
        "\n",
        "I, [Your Name], confirm that the work submitted in this assignment is my own and has been completed following academic integrity guidelines. The code is uploaded on my GitHub repository account, and the repository link is provided below:\n",
        "\n",
        "GitHub Repository Link: [Insert GitHub Link]\n",
        "\n",
        "Signature: [Full Name]"
      ],
      "metadata": {
        "id": "cox9wwws8QoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Checklist**\n",
        "\n",
        "✔ Ultralitycs Platform Documentsation Like hel file for Given Task\n",
        "\n",
        "✔ Code file (Python Notebook or Script)\n",
        "\n",
        "✔ Dataset or link to the dataset\n",
        "\n",
        "✔ Visualizations (if applicable)\n",
        "\n",
        "✔ Screenshots of model performance metrics\n",
        "\n",
        "✔ Readme File\n",
        "\n",
        "✔ Evaluation Metrics Details and discussion"
      ],
      "metadata": {
        "id": "xXk9pnhy8S2M"
      }
    }
  ]
}