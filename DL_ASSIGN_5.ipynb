{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5UI5VpsvnC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8nuCbCyvm98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1**:\n",
        "Objective: To forecast future values of a univariate time series using LSTM-based models. Group Member- 1) Vivek Borade Prn- 202201040216 2) Nirmal Chaturvedi Prn- 202201040210 3) Abhijeet Jadhav Prn- 202201040122"
      ],
      "metadata": {
        "id": "rL5-B4BPyfnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "YZO_7_fElQ2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yueXIVRqkzBi"
      },
      "outputs": [],
      "source": [
        "! pip install q kaggle\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import autoviz\n",
        "import seaborn as sns\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load Dataset"
      ],
      "metadata": {
        "id": "SnH61W2YlVqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"prasoonkottarathil/btcinusd\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "drRiG88UlZb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List files inside the dataset folder\n",
        "folder_path = \"/root/.cache/kagglehub/datasets/prasoonkottarathil/btcinusd/versions/4\"\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "print(\"Files in the folder:\")\n",
        "for file in files:\n",
        "    print(file)\n"
      ],
      "metadata": {
        "id": "DIGCmBbGzRD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"/root/.cache/kagglehub/datasets/prasoonkottarathil/btcinusd/versions/4/BTC-Hourly.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "4zX_BslAz2cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Preprocess Text Data"
      ],
      "metadata": {
        "id": "Sgvu8dJwmN8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on univariate 'close' column\n",
        "data = df[['close']].values\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Create sequences (60 timesteps)\n",
        "def create_sequences(data, sequence_length):\n",
        "    X, y = [], []\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i, 0])\n",
        "        y.append(data[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "sequence_length = 60\n",
        "X, y = create_sequences(scaled_data, sequence_length)\n",
        "\n",
        "# Reshape for LSTM input [samples, time_steps, features]\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))"
      ],
      "metadata": {
        "id": "2ZS5NiWxmRpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train-Test Split"
      ],
      "metadata": {
        "id": "w9xjZqu-mWIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]"
      ],
      "metadata": {
        "id": "4f2xg-MBmT8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Build LSTM Model"
      ],
      "metadata": {
        "id": "Y11db43FmZ2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(units=1))"
      ],
      "metadata": {
        "id": "4zZCJncpmZj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Train the Model"
      ],
      "metadata": {
        "id": "tE6Qfqr6mrAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Predict\n",
        "predicted_scaled = model.predict(X_test)\n",
        "predicted = scaler.inverse_transform(predicted_scaled)\n",
        "actual = scaler.inverse_transform(y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "xW5kRSWgmsBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Evaluate the Model"
      ],
      "metadata": {
        "id": "PXQil-oknL1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "mae = mean_absolute_error(actual, predicted)\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"MAE: {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "6Z9Jls-fnO1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Step 8: Plot actual vs predicted"
      ],
      "metadata": {
        "id": "9Juj5jCmnRTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(actual, label='Actual BTC Close Price')\n",
        "plt.plot(predicted, label='Predicted BTC Close Price')\n",
        "plt.title('BTC Price Prediction vs Actual')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PTYLOlKsnSXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v20ONWG4v3Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KB-s1V47v3Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2"
      ],
      "metadata": {
        "id": "0Tmx057l-lK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "vnYiRb9Q-spk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "MxbLU7m--iyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load Dataset"
      ],
      "metadata": {
        "id": "-HTsElG6-2XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load Shakespeare dataset from TensorFlow Datasets\n",
        "# Remove as_supervised=True\n",
        "dataset, info = tfds.load('tiny_shakespeare', with_info=True)\n",
        "train_data = dataset['train']\n",
        "\n",
        "# Get the raw text\n",
        "text = ''\n",
        "# Extract the text data from the dataset\n",
        "for example in train_data:\n",
        "    text += example['text'].numpy().decode('utf-8')\n",
        "\n",
        "# Preprocessing text: convert all to lowercase and remove unnecessary characters\n",
        "text = text.lower().replace('\\n', ' ').replace('\\r', '')\n"
      ],
      "metadata": {
        "id": "eq5oBVs4_KoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 3: Vectorize the data"
      ],
      "metadata": {
        "id": "eSzC8xpA_OLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a set of unique characters in the text\n",
        "chars = sorted(set(text))\n",
        "\n",
        "# Create a mapping from character to integer and vice versa\n",
        "char_to_index = {char: idx for idx, char in enumerate(chars)}\n",
        "index_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# Convert the entire text into a sequence of integers\n",
        "text_as_int = np.array([char_to_index[char] for char in text])"
      ],
      "metadata": {
        "id": "1dXTM6t2_Kk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Prepare Sequences for Training"
      ],
      "metadata": {
        "id": "70EvQAQS_ejV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length and batch size\n",
        "sequence_length = 100\n",
        "batch_size = 64\n",
        "\n",
        "# Generate sequences of length `sequence_length`\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(len(text_as_int) - sequence_length):\n",
        "    sequences.append(text_as_int[i:i+sequence_length])\n",
        "    next_chars.append(text_as_int[i+sequence_length])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(sequences)\n",
        "y = np.array(next_chars)"
      ],
      "metadata": {
        "id": "9G1mPpb1_Kik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Build LSTM Model"
      ],
      "metadata": {
        "id": "zJE4s1lr_lY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(chars), output_dim=256, input_length=sequence_length),\n",
        "    LSTM(512, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(512),\n",
        "    Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Uv4Rvfqu_b3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Train the Model"
      ],
      "metadata": {
        "id": "40-MmAId_0xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y, epochs=3, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "bqSV5O2D_b0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7:Plot Training Accuracy and Loss"
      ],
      "metadata": {
        "id": "oLo68_Z7AA4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy and loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Gemem2ol_byQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PwbrZ4bAIaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "md4PpXV4AIWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoxYfJ2FAIBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3"
      ],
      "metadata": {
        "id": "kV18awl0xWjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "E7jJ5jGrwEk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n"
      ],
      "metadata": {
        "id": "otpI4G22v3JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Load Dataset"
      ],
      "metadata": {
        "id": "uUjx2-cMwc0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDb dataset\n",
        "vocab_size = 10000  # Use the top 10,000 most frequent words\n",
        "maxlen = 200        # Max length of review sequences\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n"
      ],
      "metadata": {
        "id": "x8pg86xrwKqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to ensure uniform input length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(f\"Padded x_train shape: {x_train.shape}\")\n",
        "print(f\"Padded x_test shape: {x_test.shape}\")\n"
      ],
      "metadata": {
        "id": "4ia5czgfwKmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Build LSTM Model"
      ],
      "metadata": {
        "id": "WuemBbNHw9P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "VPLbP6RXwKj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 4: Train the Model"
      ],
      "metadata": {
        "id": "69KJ7EfGxJXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=5,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "dyFQ5FsZwKgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Evaluate Model"
      ],
      "metadata": {
        "id": "VUi485J3xs0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "score, acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "id": "CuvqpcUtwKeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and print classification report\n",
        "y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Negative\", \"Positive\"]))\n"
      ],
      "metadata": {
        "id": "1xc7OsJ8wKb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Confusion matrix"
      ],
      "metadata": {
        "id": "brrIR1q7x-1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O066kKKgwX98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noEvC604wX6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_AM3SPJwX4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rPyfwglqwX1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDBEOIqLwXzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}